{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## you may need to download a few things first\n",
    "#nltk.download()\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "# see https://github.com/cjhutto/vaderSentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"./datasets/processed/messages/\"\n",
    "pstemmer = PorterStemmer()\n",
    "sstemmer = SnowballStemmer(\"english\")\n",
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_corpus = PlaintextCorpusReader(corpus_path, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "n = 10\n",
    "for filename in enron_corpus.fileids():\n",
    "    if counter < n:\n",
    "        print(filename)\n",
    "        counter += 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's open up the first doc in the corpus\n",
    "raw_text = enron_corpus.raw(\"<1000115.1075852075775.JavaMail.evans@thyme>.txt\")\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text word by word\n",
    "wt = word_tokenize(raw_text, language=\"english\")\n",
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text sentence by sentence\n",
    "st = sent_tokenize(raw_text, language=\"english\")\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of typical English stopwords\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "english_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_after_stopwords_removal = [word for word in set(wt) if word not in english_stopwords]\n",
    "wt_after_stopwords_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in wt_after_stopwords_removal:\n",
    "    print(\"stem of' '%s' is '%s'(portman stemmer) and '%s'(snowball stemmer)\"\n",
    "          % (word, pstemmer.stem(word[0]), sstemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = \"\"\n",
    "for docfile in enron_corpus.fileids()[0:10]:\n",
    "    raw_texts = enron_corpus.raw(docfile)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(max_words=10, stopwords=english_stopwords,\n",
    "                      margin=10, random_state=1, max_font_size=100, \n",
    "                      background_color=\"white\").generate(raw_texts)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# lower max_font_size a little\n",
    "wordcloud = WordCloud(max_font_size=40).generate(raw_texts)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE: \n",
      "That was pretty funny .... he is doing research on NW utilities , however that is not their mandate to market to them ... so I say , then why are you doing research ... he say ' s ... \" I ' m not ... who said I was \"... it was about that time I started to laugh ... oh well . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.803, 'pos': 0.197, 'compound': 0.8957} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "----- Original Message ----- From : Calger , Christopher F . Sent : Wednesday , October 24 , 2001 10 : 32 PM To : Tycholiz , Barry Subject : FW : Natural Gas Origination \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.814, 'pos': 0.186, 'compound': 0.5859} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "I ' m glad I ' m in the west - I know who to call about gas deals ? you and ader ! \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.798, 'pos': 0.202, 'compound': 0.5093} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "----- Original Message ----- From : Enron Announcements / Corp / Enron @ ENRON On Behalf Of Enron Americas - Office of the Chairman @ ENRON Sent : Wednesday , October 24 , 2001 7 : 22 PM To : All Enron Employees North America @ ENRON Subject : Natural Gas Origination \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.5859} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "Our natural gas business continues to benefit from effective account management and resource allocation focused on identifying and responding to the needs of our varied customers . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.663, 'pos': 0.337, 'compound': 0.8807} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "In order to keep our organization optimally structured and to facilitate additional growth , we are making the following changes : \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.776, 'pos': 0.224, 'compound': 0.5994} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "Producer / Wellhead Group The current mid - market , origination and wellhead pricing activity currently within the Central and Eastern Gas Regions will be consolidated with the Derivatives group under Fred Lagrasta . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'compound': 0.0516} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "This will create a single business unit focused upon the needs of the producing industry within the Eastern U . S . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.761, 'pos': 0.239, 'compound': 0.5719} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "The producer focus in the Western U . S . and Texas will remain unchanged reporting to Mark Whitt and Brian Redmond respectively . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.882, 'pos': 0.118, 'compound': 0.34} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "Strategic Asset Development Laura Luce will move from her role in the Central Region to lead an effort focused strictly on identifying and entering into long - term strategic arrangements within the Central and Eastern Regions . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.866, 'pos': 0.134, 'compound': 0.6249} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "This initiative will focus on a limited number of selected markets that provide strategic opportunities for partnering in asset development , asset management and optimization . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.062, 'neu': 0.559, 'pos': 0.378, 'compound': 0.8225} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "This effort will continue to work very closely with the regional leads . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "Central Origination and Mid - Market Frank Vickers will continue his current role in the Eastern Region and will assume the leadership role for Mid - Market and Origination activity in the Central Region . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "There will be no changes to the West and Texas Origination groups headed respectively by Barry Tycholiz and Brian Redmond . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.097, 'neu': 0.796, 'pos': 0.106, 'compound': 0.0516} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "Please join us in congratulating Fred , Laura and Frank in their new roles . \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 0.71, 'pos': 0.29, 'compound': 0.5423} \n",
      "\n",
      "------------------------------------------------------------------------\n",
      "SENTENCE: \n",
      "Louise & John \n",
      "\n",
      "POLARITY SCORE: \n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0} \n",
      "\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# let's calculate valence score for contents of the first file in the corpus\n",
    "for sentence in enron_corpus.sents(\"<1000115.1075852075775.JavaMail.evans@thyme>.txt\"):\n",
    "    whole_sentence = \" \".join(sentence)\n",
    "    print(\"SENTENCE: \\n%s \\n\" % whole_sentence)\n",
    "    print(\"POLARITY SCORE: \\n%s \\n\" % vader_analyzer.polarity_scores(whole_sentence))\n",
    "    print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Scoring\n",
    "\n",
    "\n",
    "* The ``compound`` score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate. \n",
    " \n",
    "  It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative.  \n",
    "  Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    " #. **positive sentiment**: ``compound`` score >=  0.5\n",
    " #. **neutral  sentiment**: (``compound`` score > -0.5) and (``compound`` score < 0.5)\n",
    " #. **negative sentiment**: ``compound`` score <= -0.5\n",
    "\n",
    "* The ``pos``, ``neu``, and ``neg`` scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation).  These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load a few useful libs\n",
    "import tarfile\n",
    "import gc\n",
    "import email\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "message_save_dir = \"./datasets/processed/messages/\"\n",
    "metadata_save_dir = \"./datasets/processed/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning: Next cell may take too much time to be run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's iterate over the dataset file without opening it up on the file system\n",
    "\n",
    "counter = 0\n",
    "with tarfile.open(\"./datasets/sample_enron_mail_20150507.tar.gz\") as dataset_file:\n",
    "    for file in dataset_file.getmembers():\n",
    "        try:\n",
    "            file_content = dataset_file.extractfile(file).read()\n",
    "            parsed_message = email.message_from_string(file_content.decode(\"utf-8\"))\n",
    "            metadata = dict(parsed_message.items())\n",
    "            print(metadata['To'])\n",
    "            metadata['To'] = re.sub(r'[\\n\\r\\t ]+', '', metadata['To']).split(\",\")\n",
    "            print(metadata['To'])\n",
    "            message = parsed_message.get_payload()\n",
    "            \n",
    "            # write out the message\n",
    "            with io.open(os.path.join(message_save_dir, metadata['Message-ID']+\".txt\"), \"w\") as message_datafile:\n",
    "                message_datafile.write(message)\n",
    "            \n",
    "             # write out the message metadata(as json)\n",
    "            with io.open(os.path.join(metadata_save_dir, metadata['Message-ID']+\".json\"), \"w\") as metadata_file:\n",
    "                json.dump(metadata, metadata_file)\n",
    "                \n",
    "            #print(str(counter) + \" \" + file.name + \" processed!\")\n",
    "            \n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "        finally:\n",
    "            counter += 1\n",
    "            \n",
    "# let gc!\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warning: Run next cell just once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# don't run this cell multiple times\n",
    "metadata_files = glob.glob(os.path.join(metadata_save_dir, \"*.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "graph = dict()\n",
    "for metadata_file in metadata_files:\n",
    "    if counter < 100:\n",
    "        print(metadata_file)\n",
    "        counter += 1\n",
    "    else:\n",
    "        continue\n",
    "    with io.open(metadata_file) as metadata:\n",
    "        metadata = json.load(metadata)\n",
    "        if metadata['From'] in graph:\n",
    "            graph[metadata['From']].extend(metadata['To'])\n",
    "        else:\n",
    "            graph[metadata['From']] = (metadata['To'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open(\"./datasets/processed/enron_graph.tsv\", \"w\") as enron_graph:\n",
    "    enron_graph.write(\"source\\ttarget\\tweight\\n\")\n",
    "    for k, v in graph.items():\n",
    "        source = k\n",
    "        targets = [(target, v.count(target)) for target in v]\n",
    "        for pair in targets:\n",
    "            target, weight = pair\n",
    "            enron_graph.write(source+\"\\t\"+target+\"\\t\"+str(weight)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_graph = pandas.read_csv(\"./datasets/processed/enron_graph.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_graph.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby_people = enron_graph.groupby(by=[\"source\", \"target\"])\n",
    "\n",
    "# unique emails(a.k.a graph nodes)\n",
    "groupby_people.count().reset_index()['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " metadata['To']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "## you may need to download a few things first\n",
    "#nltk.download()\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download(\"stopwords\")\n",
    "\n",
    "# see https://github.com/cjhutto/vaderSentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = \"./datasets/processed/messages/\"\n",
    "pstemmer = PorterStemmer()\n",
    "sstemmer = SnowballStemmer(\"english\")\n",
    "vader_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enron_corpus = PlaintextCorpusReader(corpus_path, '.*\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "n = 10\n",
    "for filename in enron_corpus.fileids():\n",
    "    if counter < n:\n",
    "        print(filename)\n",
    "        counter += 1\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's open up the first doc in the corpus\n",
    "raw_text = enron_corpus.raw(\"<1000115.1075852075775.JavaMail.evans@thyme>.txt\")\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text word by word\n",
    "wt = word_tokenize(raw_text, language=\"english\")\n",
    "wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the text sentence by sentence\n",
    "st = sent_tokenize(raw_text, language=\"english\")\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of typical English stopwords\n",
    "english_stopwords = stopwords.words(\"english\")\n",
    "english_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_after_stopwords_removal = [word for word in set(wt) if word not in english_stopwords]\n",
    "wt_after_stopwords_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in wt_after_stopwords_removal:\n",
    "    print(\"stem of' '%s' is '%s'(portman stemmer) and '%s'(snowball stemmer)\"\n",
    "          % (word, pstemmer.stem(word[0]), sstemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_texts = \"\"\n",
    "for docfile in enron_corpus.fileids()[0:10]:\n",
    "    raw_texts = enron_corpus.raw(docfile)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud(max_words=10, stopwords=english_stopwords,\n",
    "                      margin=10, random_state=1, max_font_size=100, \n",
    "                      background_color=\"white\").generate(raw_texts)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# lower max_font_size a little\n",
    "wordcloud = WordCloud(max_font_size=40).generate(raw_texts)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's calculate valence score for contents of the first file in the corpus\n",
    "for sentence in enron_corpus.sents(\"<1000115.1075852075775.JavaMail.evans@thyme>.txt\"):\n",
    "    whole_sentence = \" \".join(sentence)\n",
    "    print(\"SENTENCE: \\n%s \\n\" % whole_sentence)\n",
    "    print(\"POLARITY SCORE: \\n%s \\n\" % vader_analyzer.polarity_scores(whole_sentence))\n",
    "    print(\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Scoring\n",
    "\n",
    "\n",
    "* The ``compound`` score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate. \n",
    " \n",
    "  It is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative.  \n",
    "  Typical threshold values (used in the literature cited on this page) are:\n",
    "\n",
    " #. **positive sentiment**: ``compound`` score >=  0.5\n",
    " #. **neutral  sentiment**: (``compound`` score > -0.5) and (``compound`` score < 0.5)\n",
    " #. **negative sentiment**: ``compound`` score <= -0.5\n",
    "\n",
    "* The ``pos``, ``neu``, and ``neg`` scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation).  These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
